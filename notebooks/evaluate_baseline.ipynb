{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Parsed Arguments\n",
      "DATASET_PATH: /home/mila/a/armin.moradi/scratch/data/LFM_2b_seperated_final\n",
      "LOGS_PATH: /home/mila/a/armin.moradi/CulturalDiscoverability/results/model_outputs_testing_ipynb/\n",
      "--------------------------------------------------\n",
      "Loaded Data interactions: 10520266 users 9856 items 6321172\n",
      "--------------------------------------------------\n",
      "Sampled User Demographics: 50\n",
      "59397 len of sampled user interactions\n",
      "Added Negative Samples ~ Added Items: 59397\n",
      "--------------------------------------------------\n",
      "DF FINAL - n rows: 118794\n",
      "--------------------------------------------------\n",
      "Finished Pre-processing\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import argparse\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device:', device)\n",
    "\n",
    "# # wandb\n",
    "# wandb.init(project='cultural-rs', entity='armornine')\n",
    "# print('Initialized WandB')\n",
    "\n",
    "# # parse args\n",
    "# parser = argparse.ArgumentParser(description=\"Argparser for modifying the training settings\")\n",
    "# parser.add_argument(\"--logs_path\", type=str, help=\"Path to save outputs\")\n",
    "# parser.add_argument(\"--dataset_path\", type=str, help=\"Path to read datasets\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# DATASET_PATH = args.dataset_path\n",
    "# LOGS_PATH = args.logs_path\n",
    "\n",
    "DATASET_PATH = '/home/mila/a/armin.moradi/scratch/data/LFM_2b_seperated_final'\n",
    "LOGS_PATH = '/home/mila/a/armin.moradi/CulturalDiscoverability/results/model_outputs_testing_ipynb/'\n",
    "\n",
    "print('Parsed Arguments')\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('LOGS_PATH:', LOGS_PATH)\n",
    "print('-'*50)\n",
    "\n",
    "# load data\n",
    "\n",
    "interactions = pd.read_csv(os.path.join(DATASET_PATH, '10k_sampled_interactions.csv'))\n",
    "user_demographics = pd.read_csv(os.path.join(DATASET_PATH, '10k_sampled_users.csv'))\n",
    "item_demographics = pd.read_csv(os.path.join(DATASET_PATH, 'item_demographics.csv'))\n",
    "\n",
    "print('Loaded Data', 'interactions:', len(interactions), 'users', len(user_demographics), 'items', len(item_demographics))\n",
    "print('-'*50)\n",
    "\n",
    "interactions['count'] = 1\n",
    "\n",
    "# reindex IDs\n",
    "\n",
    "user_demographics['new_user_id'] = user_demographics.index\n",
    "user_demographics = user_demographics.sample(n=50, random_state=99) # TODO: CHANGE TO COMPLETE\n",
    "print('Sampled User Demographics:', len(user_demographics))\n",
    "\n",
    "df_final = interactions.merge(user_demographics, on='user_id', how='inner')\n",
    "\n",
    "print(len(df_final), 'len of sampled user interactions')\n",
    "\n",
    "# add random negative samples\n",
    "\n",
    "user_ids = df_final['user_id'].unique()\n",
    "item_ids = interactions['track_id'].unique()\n",
    "\n",
    "negative_samples = []\n",
    "for user_id in user_ids:\n",
    "    user_interacted_items = interactions[interactions['user_id'] == user_id]['track_id'].values\n",
    "    user_negative_items = np.setdiff1d(item_ids, user_interacted_items)\n",
    "    user_negative_items = np.random.choice(user_negative_items, size=len(user_interacted_items), replace=False)\n",
    "    for negative_item in user_negative_items:\n",
    "        negative_samples.append([user_id, negative_item, 0])\n",
    "\n",
    "negative_interactions = pd.DataFrame(negative_samples, columns=['user_id', 'track_id', 'count'])\n",
    "negative_df = negative_interactions.merge(user_demographics, on='user_id', how='inner')\n",
    "df_final = pd.concat([df_final, negative_df], ignore_index=True)\n",
    "\n",
    "print('Added Negative Samples', '~ Added Items:', len(df_final[df_final['count'] == 0]))\n",
    "print('-'*50)\n",
    "\n",
    "item_demographics['new_track_id'] = item_demographics.index\n",
    "item_demographics = item_demographics[item_demographics['track_id'].isin(interactions['track_id'])]\n",
    "item_demographics = item_demographics.sample(frac=1, random_state=99) #TODO: CHANGE TO COMPLETE\n",
    "\n",
    "df_final = df_final.merge(item_demographics, on='track_id', how='inner')\n",
    "\n",
    "print('DF FINAL - n rows:', len(df_final))\n",
    "print('-'*50)\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "labels = ['0-18', '18-30', '30-50', '50+']\n",
    "bins = [0, 18, 30, 50, 100]\n",
    "df_final['user_age'] = pd.cut(df_final['user_age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# replace user id with new_user_id and delete user_id\n",
    "df_final = df_final.drop(columns=['user_id'])\n",
    "df_final = df_final.rename(columns={'new_user_id':'user_id'})\n",
    "\n",
    "# replace user id with new_user_id and delete user_id\n",
    "df_final = df_final.drop(columns=['track_id'])\n",
    "df_final = df_final.rename(columns={'new_track_id':'track_id'})\n",
    "\n",
    "cols = df_final.columns.tolist()\n",
    "cols = [cols[-1], cols[3]] + cols[0:3] + cols[4:-1]\n",
    "df_final = df_final[cols]\n",
    "print('Finished Pre-processing')\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m df_final\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m columns_to_encode \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m      7\u001b[0m encoder\u001b[38;5;241m.\u001b[39mfit(x[columns_to_encode])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "# shape data\n",
    "\n",
    "y = df_final['count']\n",
    "x = df_final.drop(['count'], axis=1)\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "columns_to_encode = x.columns[2:]\n",
    "encoder.fit(x[columns_to_encode])\n",
    "encoded_x = encoder.transform(x[columns_to_encode])\n",
    "encoded_x = pd.DataFrame(encoded_x, columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "x = pd.concat([x[['user_id', 'track_id']], encoded_x], axis=1)\n",
    "\n",
    "user_country_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_country_')]\n",
    "artist_country_indices = [i for i, col in enumerate(x.columns) if col.startswith('artist_country_')]\n",
    "user_gender_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_gender_')]\n",
    "artist_gender_indices = [i for i, col in enumerate(x.columns) if col.startswith('artist_gender')]\n",
    "user_age_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_age_')]\n",
    "unique_user_ids, unique_track_ids = max(x['user_id']) + 1, max(x['track_id']) + 1\n",
    "\n",
    "# split the interactions of each user into train, val, and test (80-10-10)\n",
    "train_share = 0.8\n",
    "val_share = 0.1\n",
    "test_share = 1 - train_share - val_share\n",
    "\n",
    "user_interactions = {}\n",
    "for i in range(len(x)):\n",
    "    user_id = x.iloc[i]['user_id']\n",
    "    if user_id not in user_interactions:\n",
    "        user_interactions[user_id] = []\n",
    "    user_interactions[user_id].append(i)\n",
    "\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "for user_id, indices in user_interactions.items():\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices += indices[:int(train_share * len(indices))]\n",
    "    val_indices += indices[int(train_share * len(indices)):int((train_share + val_share ) * len(indices))]\n",
    "    test_indices += indices[int((1 - test_share) * len(indices)):]\n",
    "    \n",
    "train_x, train_y = x.iloc[train_indices], y.iloc[train_indices]\n",
    "val_x, val_y = x.iloc[val_indices], y.iloc[val_indices]\n",
    "test_x, test_y = x.iloc[test_indices], y.iloc[test_indices]\n",
    "\n",
    "# create dataloaders\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))\n",
    "val_dataset = TensorDataset(torch.tensor(val_x.values, dtype=torch.float), torch.tensor(val_y.values, dtype=torch.float))\n",
    "test_dataset = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f'created loaders with batchsize {batch_size}', len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/30 [21:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mfloat(), yy\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     65\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 66\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# wandb.log({'train_loss': total_loss / len(train_loader)})    \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training model with only user_id and track_id and countries\n",
    "\n",
    "class AgnosticNCF(nn.Module):\n",
    "    def __init__(self, num_users, num_tracks, user_country_dim, artist_country_dim, hidden_size=[256, 128, 64]):\n",
    "        super(AgnosticNCF, self).__init__()\n",
    "\n",
    "        self.embedding_size = 32\n",
    "\n",
    "        self.user_id_embedding = nn.Embedding(num_users, self.embedding_size)\n",
    "        self.track_id_embedding = nn.Embedding(num_tracks, self.embedding_size)\n",
    "\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size * 2, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[2], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, artist_id, user_country, artist_country):\n",
    "\n",
    "        user_id_embeds = self.user_id_embedding(user_id)\n",
    "        track_id_embeds = self.track_id_embedding(artist_id)\n",
    "        \n",
    "\n",
    "        concatenated = torch.cat([user_id_embeds, track_id_embeds], dim=1)\n",
    "        output = self.fc_layers(concatenated.float())\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "print('Start Training')\n",
    "print('-'*50)\n",
    "\n",
    "start_time = time.time()\n",
    "model = AgnosticNCF(num_users=unique_user_ids, num_tracks=unique_track_ids,\n",
    "                                    user_country_dim=len(user_country_indices),\n",
    "                                    artist_country_dim=len(artist_country_indices),\n",
    "                                    hidden_size=[64, 128, 64]).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xx, yy in train_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        user_country = xx[:, user_country_indices].long().to(device)\n",
    "        artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_id, track_id, user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # wandb.log({'train_loss': total_loss / len(train_loader)})    \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    for xx, yy in val_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        user_country = xx[:, user_country_indices].long().to(device)\n",
    "        artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id, user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_val_loss += loss.item()\n",
    "    # wandb.log({'val_loss': total_val_loss / len(val_loader)})        \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}, Val Loss: {total_val_loss / len(val_loader)}')\n",
    "    train_loss.append(total_loss / len(train_loader))\n",
    "    val_loss.append(total_val_loss / len(val_loader))\n",
    "\n",
    "    # torch.save(model.state_dict(), LOGS_PATH + 'model_weights.pth')\n",
    "\n",
    "print('Training Time:', round((time.time() - start_time)/60, 2), 'minutes')\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 1/30 [00:22<11:06, 22.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.3442390374839306, Val Loss: 0.2524240016937256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 2/30 [00:44<10:23, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Loss: 0.2538522370159626, Val Loss: 0.26560235023498535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 3/30 [01:06<09:56, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 0.24381409212946892, Val Loss: 0.25064876675605774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 4/30 [01:29<09:38, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Loss: 0.23339933343231678, Val Loss: 0.2569940984249115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 5/30 [01:52<09:22, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss: 0.2363328691571951, Val Loss: 0.25869473814964294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 6/30 [02:13<08:54, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 0.22948197275400162, Val Loss: 0.24573026597499847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 7/30 [02:35<08:30, 22.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Loss: 0.21813501045107841, Val Loss: 0.2645861506462097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 8/30 [02:57<08:05, 22.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Loss: 0.20902347937226295, Val Loss: 0.24728156626224518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 9/30 [03:20<07:47, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 0.19430579245090485, Val Loss: 0.24185332655906677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 10/30 [03:43<07:29, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.18881103210151196, Val Loss: 0.2558070719242096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 11/30 [04:06<07:11, 22.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Loss: 0.1674036905169487, Val Loss: 0.2487363964319229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 12/30 [04:28<06:45, 22.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 0.16193882562220097, Val Loss: 0.2853354811668396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 13/30 [04:52<06:30, 22.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Loss: 0.16457091085612774, Val Loss: 0.25575822591781616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 14/30 [05:18<06:21, 23.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Loss: 0.13804347347468138, Val Loss: 0.274686723947525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 15/30 [05:41<05:52, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 0.1358207892626524, Val Loss: 0.27396443486213684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 15/30 [06:00<06:00, 24.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mfloat(), yy\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     69\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 70\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# wandb.log({'train_loss': total_loss / len(train_loader)})    \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training model with only user_id and track_id and countries\n",
    "\n",
    "class NeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(self, num_users, num_tracks, user_country_dim, artist_country_dim, hidden_size=[256, 128, 64]):\n",
    "        super(NeuralCollaborativeFiltering, self).__init__()\n",
    "\n",
    "        self.embedding_size = 32\n",
    "\n",
    "        self.user_id_embedding = nn.Embedding(num_users, self.embedding_size)\n",
    "        self.track_id_embedding = nn.Embedding(num_tracks, self.embedding_size)\n",
    "\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size * 2, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[2], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, artist_id, user_country, artist_country):\n",
    "\n",
    "        user_id_embeds = self.user_id_embedding(user_id)\n",
    "        track_id_embeds = self.track_id_embedding(artist_id)\n",
    "        \n",
    "        # user_country_embeds = self.user_country_embedding(user_country)\n",
    "        # artist_country_embeds = self.artist_country_embedding(artist_country)\n",
    "\n",
    "        concatenated = torch.cat([user_id_embeds, track_id_embeds, user_country, artist_country], dim=1)\n",
    "        output = self.fc_layers(concatenated.float())\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "print('Start Training')\n",
    "print('-'*50)\n",
    "\n",
    "start_time = time.time()\n",
    "model = NeuralCollaborativeFiltering(num_users=unique_user_ids, num_tracks=unique_track_ids,\n",
    "                                    user_country_dim=len(user_country_indices),\n",
    "                                    artist_country_dim=len(artist_country_indices),\n",
    "                                    hidden_size=[64, 128, 64]).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xx, yy in train_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        user_country = xx[:, user_country_indices].long().to(device)\n",
    "        artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_id, track_id, user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # wandb.log({'train_loss': total_loss / len(train_loader)})    \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    for xx, yy in val_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        user_country = xx[:, user_country_indices].long().to(device)\n",
    "        artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id, user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_val_loss += loss.item()\n",
    "    # wandb.log({'val_loss': total_val_loss / len(val_loader)})        \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}, Val Loss: {total_val_loss / len(val_loader)}')\n",
    "    train_loss.append(total_loss / len(train_loader))\n",
    "    val_loss.append(total_val_loss / len(val_loader))\n",
    "\n",
    "    # torch.save(model.state_dict(), LOGS_PATH + 'model_weights.pth')\n",
    "\n",
    "print('Training Time:', round((time.time() - start_time)/60, 2), 'minutes')\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "diff_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xx, yy in test_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        user_country = xx[:, user_country_indices].long().to(device)\n",
    "        artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id, user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_test_loss += loss.item()\n",
    "        diff = abs(outputs - yy.float().to(device))\n",
    "        diff_list.append(diff)\n",
    "        wandb.log({'diff mean': diff.mean().item()})\n",
    "        wandb.log({'diff std': diff.std().item()})\n",
    "    wandb.log({'test_loss': total_test_loss / len(test_loader)})\n",
    "    \n",
    "mean_diff = torch.cat(diff_list).mean().item()\n",
    "std_diff = torch.cat(diff_list).std().item()\n",
    "\n",
    "print(f'Test Loss: {total_test_loss / len(test_loader)}')\n",
    "print(f'Mean Difference: {mean_diff}')\n",
    "print(f'Standard Deviation of Difference: {std_diff}')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the two papers (demographic info helps?, globalization?)\n",
    "\n",
    "# embedding of ids of different people\n",
    "# also learning a model w/o the demog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0]) torch.Size([0]) torch.Size([1]) torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m user_id, artist_id, user_country_tensor, artist_country_tensor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# gr ca\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crs_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mNeuralCollaborativeFiltering.forward\u001b[0;34m(self, user_id, artist_id, user_country, artist_country)\u001b[0m\n\u001b[1;32m     31\u001b[0m artist_id_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_id_embedding(artist_id)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# user_country_embeds = self.user_country_embedding(user_country)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# artist_country_embeds = self.artist_country_embedding(artist_country)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martist_id_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_country\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martist_country\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers(concatenated\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "def predict(user_id, artist_id, user_country, artist_country): # move this into the class of model later\n",
    "\n",
    "    user_id = torch.Tensor(user_id).long()\n",
    "    artist_id = torch.Tensor(artist_id).long()\n",
    "\n",
    "    user_country_idx = x.columns.get_loc('user_country_' + user_country)\n",
    "    artist_country_idx = x.columns.get_loc('artist_country_' + artist_country)\n",
    "    \n",
    "    user_country_tensor = torch.tensor(user_country_idx).unsqueeze(0)\n",
    "    artist_country_tensor = torch.tensor(artist_country_idx).unsqueeze(0)\n",
    "    \n",
    "    print(user_id.shape, artist_id.shape, user_country_tensor.shape, artist_country_tensor.shape)\n",
    "    return user_id, artist_id, user_country_tensor, artist_country_tensor\n",
    "# gr ca\n",
    "loaded_model(*predict(0, 0, 'GR', 'CA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results/NCF.pth'\n",
    "torch.save(model, model_path)\n",
    "\n",
    "loaded_model = torch.load(model_path)\n",
    "model_weights = loaded_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_user_id</th>\n",
       "      <th>old_item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>30937487</td>\n",
       "      <td>2020-02-20 07:10:29</td>\n",
       "      <td>1980</td>\n",
       "      <td>11859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25099053</td>\n",
       "      <td>2020-02-20 19:00:33</td>\n",
       "      <td>1980</td>\n",
       "      <td>26376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13081182</td>\n",
       "      <td>2020-02-20 22:27:40</td>\n",
       "      <td>1980</td>\n",
       "      <td>33715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>18298303</td>\n",
       "      <td>2020-02-20 22:37:26</td>\n",
       "      <td>1980</td>\n",
       "      <td>37099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>38104053</td>\n",
       "      <td>2020-02-20 22:42:44</td>\n",
       "      <td>1980</td>\n",
       "      <td>35117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_user_id  old_item_id            timestamp  user_id  item_id\n",
       "0            2     30937487  2020-02-20 07:10:29     1980    11859\n",
       "1            2     25099053  2020-02-20 19:00:33     1980    26376\n",
       "2            2     13081182  2020-02-20 22:27:40     1980    33715\n",
       "3            2     18298303  2020-02-20 22:37:26     1980    37099\n",
       "4            2     38104053  2020-02-20 22:42:44     1980    35117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATASET_PATH = '/home/mila/a/armin.moradi/CulturalDiscoverability/ProtoMF/data/lfm2b-1mon'\n",
    "interactions = pd.read_csv(os.path.join(DATASET_PATH, 'listening_history_train.csv'))\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data interactions: 1485540 users 8418 items 63076\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import argparse\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "\n",
    "DATASET_PATH = '/home/mila/a/armin.moradi/ProtoMF/data/lfm2b-1mon'\n",
    "ITEM_FRACTION = 1.0\n",
    "USER_FRACTION = 1.0\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "HIDDEN_SIZE = [16, 32, 64]\n",
    "EMBEDDING_SIZE = 8\n",
    "\n",
    "\n",
    "\n",
    "DATASET_PATH = '/home/mila/a/armin.moradi/CulturalDiscoverability/ProtoMF/data/lfm2b-1mon'\n",
    "interactions = pd.read_csv(os.path.join(DATASET_PATH, 'listening_history_train.csv'))\n",
    "user_demographics = pd.read_csv(os.path.join(DATASET_PATH, 'user_ids.csv'))\n",
    "item_demographics = pd.read_csv(os.path.join(DATASET_PATH, 'item_ids.csv'))\n",
    "\n",
    "print('Loaded Data', 'interactions:', len(interactions), 'users', len(user_demographics), 'items', len(item_demographics))\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980</td>\n",
       "      <td>11859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980</td>\n",
       "      <td>26376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980</td>\n",
       "      <td>33715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980</td>\n",
       "      <td>37099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>35117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  count\n",
       "0     1980    11859      1\n",
       "1     1980    26376      1\n",
       "2     1980    33715      1\n",
       "3     1980    37099      1\n",
       "4     1980    35117      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = interactions[['user_id', 'item_id']]\n",
    "interactions['count'] = 1\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>old_item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17944951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30711512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18610221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18744029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11059050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  old_item_id\n",
       "0        0     17944951\n",
       "1        1     30711512\n",
       "2        2     18610221\n",
       "3        3     18744029\n",
       "4        4     11059050"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>old_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>112165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  old_user_id\n",
       "0        0       112165\n",
       "1        1         4503\n",
       "2        2        26487\n",
       "3        3        29124\n",
       "4        4        10084"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_demographics['new_user_id'] = user_demographics.index\n",
    "# user_demographics = user_demographics.sample(frac=USER_FRACTION, random_state=99) # TODO: CHANGE TO COMPLETE\n",
    "# print('Sampled User Demographics:', len(user_demographics))\n",
    "      \n",
    "# df_final = interactions.merge(user_demographics, left_on='user_id', right_on='new_user_id', how='inner', )\n",
    "\n",
    "# print(len(df_final), 'len of sampled user interactions')\n",
    "# # add random negative samples\n",
    "\n",
    "df_final = interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980</td>\n",
       "      <td>11859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980</td>\n",
       "      <td>26376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980</td>\n",
       "      <td>33715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980</td>\n",
       "      <td>37099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>35117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  count\n",
       "0     1980    11859      1\n",
       "1     1980    26376      1\n",
       "2     1980    33715      1\n",
       "3     1980    37099      1\n",
       "4     1980    35117      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Negative Samples ~ Added Items: 1485540\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_ids = df_final['user_id'].unique()\n",
    "item_ids = interactions['item_id'].unique()\n",
    "\n",
    "negative_samples = []\n",
    "for user_id in user_ids:\n",
    "    user_interacted_items = interactions[interactions['user_id'] == user_id]['item_id'].values\n",
    "    user_negative_items = np.setdiff1d(item_ids, user_interacted_items)\n",
    "    user_negative_items = np.random.choice(user_negative_items, size=len(user_interacted_items), replace=False)\n",
    "    for negative_item in user_negative_items:\n",
    "        negative_samples.append([user_id, negative_item, 0])\n",
    "\n",
    "negative_interactions = pd.DataFrame(negative_samples, columns=['user_id', 'item_id', 'count'])\n",
    "# negative_df = negative_interactions.merge(user_demographics, on='user_id', how='inner')\n",
    "df_final = pd.concat([df_final, negative_interactions], ignore_index=True)\n",
    "\n",
    "print('Added Negative Samples', '~ Added Items:', len(df_final[df_final['count'] == 0]))\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980</td>\n",
       "      <td>11859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980</td>\n",
       "      <td>26376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980</td>\n",
       "      <td>33715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980</td>\n",
       "      <td>37099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>35117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  count\n",
       "0     1980    11859      1\n",
       "1     1980    26376      1\n",
       "2     1980    33715      1\n",
       "3     1980    37099      1\n",
       "4     1980    35117      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # drop old_user_id \n",
    "df_final = df_final.drop(columns=['old_user_id'])\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Item Demographics: 63076\n",
      "DF FINAL - n rows: 2971080\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_user_id_x</th>\n",
       "      <th>old_item_id_x</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>old_user_id_y</th>\n",
       "      <th>new_user_id</th>\n",
       "      <th>count</th>\n",
       "      <th>old_user_id</th>\n",
       "      <th>old_item_id_y</th>\n",
       "      <th>new_track_id</th>\n",
       "      <th>new_item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>30937487.0</td>\n",
       "      <td>2020-02-20 07:10:29</td>\n",
       "      <td>1980</td>\n",
       "      <td>11859</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30937487</td>\n",
       "      <td>11859</td>\n",
       "      <td>11859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.0</td>\n",
       "      <td>30937487.0</td>\n",
       "      <td>2020-03-19 22:07:55</td>\n",
       "      <td>1005</td>\n",
       "      <td>11859</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30937487</td>\n",
       "      <td>11859</td>\n",
       "      <td>11859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>590.0</td>\n",
       "      <td>30937487.0</td>\n",
       "      <td>2020-03-14 13:55:10</td>\n",
       "      <td>116</td>\n",
       "      <td>11859</td>\n",
       "      <td>590.0</td>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30937487</td>\n",
       "      <td>11859</td>\n",
       "      <td>11859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2035.0</td>\n",
       "      <td>30937487.0</td>\n",
       "      <td>2020-03-09 12:56:19</td>\n",
       "      <td>2899</td>\n",
       "      <td>11859</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>2899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30937487</td>\n",
       "      <td>11859</td>\n",
       "      <td>11859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3109.0</td>\n",
       "      <td>30937487.0</td>\n",
       "      <td>2020-02-21 15:33:26</td>\n",
       "      <td>4063</td>\n",
       "      <td>11859</td>\n",
       "      <td>3109.0</td>\n",
       "      <td>4063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30937487</td>\n",
       "      <td>11859</td>\n",
       "      <td>11859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_user_id_x  old_item_id_x            timestamp  user_id  item_id  \\\n",
       "0            2.0     30937487.0  2020-02-20 07:10:29     1980    11859   \n",
       "1           66.0     30937487.0  2020-03-19 22:07:55     1005    11859   \n",
       "2          590.0     30937487.0  2020-03-14 13:55:10      116    11859   \n",
       "3         2035.0     30937487.0  2020-03-09 12:56:19     2899    11859   \n",
       "4         3109.0     30937487.0  2020-02-21 15:33:26     4063    11859   \n",
       "\n",
       "   old_user_id_y  new_user_id  count  old_user_id  old_item_id_y  \\\n",
       "0            2.0         1980    NaN          NaN       30937487   \n",
       "1           66.0         1005    NaN          NaN       30937487   \n",
       "2          590.0          116    NaN          NaN       30937487   \n",
       "3         2035.0         2899    NaN          NaN       30937487   \n",
       "4         3109.0         4063    NaN          NaN       30937487   \n",
       "\n",
       "   new_track_id  new_item_id  \n",
       "0         11859        11859  \n",
       "1         11859        11859  \n",
       "2         11859        11859  \n",
       "3         11859        11859  \n",
       "4         11859        11859  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item_demographics['new_item_id'] = item_demographics.index\n",
    "# item_demographics = item_demographics[item_demographics['item_id'].isin(interactions['item_id'])]\n",
    "# item_demographics = item_demographics.sample(frac=ITEM_FRACTION, random_state=99) #TODO: CHANGE TO COMPLETE\n",
    "# print('Sampled Item Demographics:', len(item_demographics))\n",
    "\n",
    "# df_final = df_final.merge(item_demographics, on='item_id', how='inner')\n",
    "\n",
    "# print('DF FINAL - n rows:', len(df_final))\n",
    "# print('-'*50)\n",
    "\n",
    "# df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2971075</th>\n",
       "      <td>8113</td>\n",
       "      <td>33845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971076</th>\n",
       "      <td>8113</td>\n",
       "      <td>7445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971077</th>\n",
       "      <td>8113</td>\n",
       "      <td>16254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971078</th>\n",
       "      <td>8113</td>\n",
       "      <td>6909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971079</th>\n",
       "      <td>8113</td>\n",
       "      <td>1188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  count\n",
       "2971075     8113    33845      0\n",
       "2971076     8113     7445      0\n",
       "2971077     8113    16254      0\n",
       "2971078     8113     6909      0\n",
       "2971079     8113     1188      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  count\n",
       "0        0        0      1\n",
       "1        0       38      0\n",
       "2        0       51      0\n",
       "3        0       65      1\n",
       "4        0       86      0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add up the columns with repeated user_id and item_id and count\n",
    "df_final = df_final.groupby(['user_id', 'item_id']).sum().reset_index()\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, item_id, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['0-18', '18-30', '30-50', '50+']\n",
    "bins = [0, 18, 30, 50, 100]\n",
    "df_final['user_age'] = pd.cut(df_final['user_age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# replace user id with new_user_id and delete user_id\n",
    "df_final = df_final.drop(columns=['user_id'])\n",
    "df_final = df_final.rename(columns={'new_user_id':'user_id'})\n",
    "\n",
    "# replace user id with new_user_id and delete user_id\n",
    "df_final = df_final.drop(columns=['track_id'])\n",
    "df_final = df_final.rename(columns={'new_track_id':'track_id'})\n",
    "\n",
    "cols = df_final.columns.tolist()\n",
    "cols = [cols[-1], cols[3]] + cols[0:3] + cols[4:-1]\n",
    "df_final = df_final[cols]\n",
    "print('Finished Pre-processing')\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "# shape data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>0</td>\n",
       "      <td>44802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0</td>\n",
       "      <td>48803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0</td>\n",
       "      <td>43312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>8281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>16501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  item_id\n",
       "660        0    44802\n",
       "720        0    48803\n",
       "639        0    43312\n",
       "145        0     8281\n",
       "280        0    16501"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_final['count']\n",
    "x = df_final.drop(['count'], axis=1)\n",
    "encoder = OneHotEncoder()\n",
    "# columns_to_encode = x.columns[2:]\n",
    "# encoder.fit(x[columns_to_encode])\n",
    "# encoded_x = encoder.transform(x[columns_to_encode])\n",
    "# encoded_x = pd.DataFrame(encoded_x, columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "# x = pd.concat([x[['user_id', 'track_id']], encoded_x], axis=1)\n",
    "\n",
    "# user_country_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_country_')]\n",
    "# artist_country_indices = [i for i, col in enumerate(x.columns) if col.startswith('artist_country_')]\n",
    "# user_gender_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_gender_')]\n",
    "# artist_gender_indices = [i for i, col in enumerate(x.columns) if col.startswith('artist_gender')]\n",
    "# user_age_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_age_')]\n",
    "# unique_user_ids, unique_track_ids = max(x['user_id']) + 1, max(x['track_id']) + 1\n",
    "\n",
    "# split the interactions of each user into train, val, and test (80-10-10)\n",
    "\n",
    "train_share = 0.75\n",
    "val_share = 0.15\n",
    "test_share = 1 - train_share - val_share\n",
    "\n",
    "user_interactions = {}\n",
    "for i in range(len(x)):\n",
    "    user_id = x.iloc[i]['user_id']\n",
    "    if user_id not in user_interactions:\n",
    "        user_interactions[user_id] = []\n",
    "    user_interactions[user_id].append(i)\n",
    "\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "for user_id, indices in user_interactions.items():\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices += indices[:int(train_share * len(indices))]\n",
    "    val_indices += indices[int(train_share * len(indices)):int((train_share + val_share ) * len(indices))]\n",
    "    test_indices += indices[int((1 - test_share) * len(indices)):]\n",
    "    \n",
    "train_x, train_y = x.iloc[train_indices], y.iloc[train_indices]\n",
    "val_x, val_y = x.iloc[val_indices], y.iloc[val_indices]\n",
    "test_x, test_y = x.iloc[test_indices], y.iloc[test_indices]\n",
    "\n",
    "\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created loaders with batchsize 32 2226217 444421 300442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create dataloaders\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))\n",
    "val_dataset = TensorDataset(torch.tensor(val_x.values, dtype=torch.float), torch.tensor(val_y.values, dtype=torch.float))\n",
    "test_dataset = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f'created loaders with batchsize {BATCH_SIZE}', len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8af8e06d30>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8ad5959160, raw_cell=\"# set up wandb logging\n",
      "\n",
      "wandb.init(project='cultur..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcn-h002.server.mila.quebec/home/mila/a/armin.moradi/CulturalDiscoverability/notebooks/evaluate_baseline.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i0c5jskg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 532749... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sparkling-rat-1</strong>: <a href=\"https://wandb.ai/armornine/foolin-around/runs/i0c5jskg\" target=\"_blank\">https://wandb.ai/armornine/foolin-around/runs/i0c5jskg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20240213_141418-i0c5jskg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i0c5jskg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/armornine/cultural-rs/runs/3v3tuzei\" target=\"_blank\">glistening-rocket-54</a></strong> to <a href=\"https://wandb.ai/armornine/cultural-rs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized WandB\n"
     ]
    }
   ],
   "source": [
    "# set up wandb logging\n",
    "\n",
    "wandb.init(project='cultural-rs', entity='armornine')\n",
    "print('Initialized WandB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = df_final['user_id'].nunique()\n",
    "unique_track_ids = df_final['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [03:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'user_country' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 71\u001b[0m\n\u001b[1;32m     66\u001b[0m track_id \u001b[38;5;241m=\u001b[39m xx[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# user_country = xx[:, user_country_indices].long().to(device)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# artist_country = xx[:, artist_country_indices].long().to(device)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(user_id, track_id, \u001b[43muser_country\u001b[49m, artist_country)\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mfloat(), yy\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     73\u001b[0m total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_country' is not defined"
     ]
    }
   ],
   "source": [
    "class AgnosticNCF(nn.Module):\n",
    "    def __init__(self, num_users, num_tracks, hidden_size=[16, 32, 32]): # 256, 128, 64\n",
    "        super(AgnosticNCF, self).__init__()\n",
    "\n",
    "        self.embedding_size = 8 # 32\n",
    "\n",
    "        self.user_id_embedding = nn.Embedding(num_users, self.embedding_size)\n",
    "        self.track_id_embedding = nn.Embedding(num_tracks, self.embedding_size)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size * 2, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[2], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, artist_id):\n",
    "\n",
    "        user_id_embeds = self.user_id_embedding(user_id)\n",
    "        track_id_embeds = self.track_id_embedding(artist_id)\n",
    "        \n",
    "\n",
    "        concatenated = torch.cat([user_id_embeds, track_id_embeds], dim=1)\n",
    "        output = self.fc_layers(concatenated.float())\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "print('Start Training')\n",
    "print('-'*50)\n",
    "\n",
    "start_time = time.time()\n",
    "model = AgnosticNCF(num_users=unique_user_ids, num_tracks=unique_track_ids, hidden_size=[64, 128, 64]).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xx, yy in train_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        # user_country = xx[:, user_country_indices].long().to(device)\n",
    "        # artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_id, track_id)# , user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    wandb.log({'train_loss': total_loss / len(train_loader)})\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    for xx, yy in val_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        # user_country = xx[:, user_country_indices].long().to(device)\n",
    "        # artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_val_loss += loss.item()\n",
    "    wandb.log({'val_loss': total_val_loss / len(val_loader)})\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(train_loader)}, Val Loss: {total_val_loss / len(val_loader)}')\n",
    "    train_loss.append(total_loss / len(train_loader))\n",
    "    val_loss.append(total_val_loss / len(val_loader))\n",
    "\n",
    "    torch.save(model.state_dict(), LOGS_PATH + 'model_weights.pth')\n",
    "\n",
    "print('Training Time:', round((time.time() - start_time)/60, 2), 'minutes')\n",
    "print('-'*50)\n",
    "\n",
    "# test\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "diff_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xx, yy in test_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        # user_country = xx[:, user_country_indices].long().to(device)\n",
    "        # artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_test_loss += loss.item()\n",
    "        diff = abs(outputs - yy.float().to(device))\n",
    "        diff_list.append(diff)\n",
    "        wandb.log({'diff mean': diff.mean().item()})\n",
    "        wandb.log({'diff std': diff.std().item()})\n",
    "    wandb.log({'test_loss': total_test_loss / len(test_loader)})\n",
    "    \n",
    "mean_diff = torch.cat(diff_list).mean().item()\n",
    "std_diff = torch.cat(diff_list).std().item()\n",
    "\n",
    "print(f'Test Loss: {total_test_loss / len(test_loader)}')\n",
    "print(f'Mean Difference: {mean_diff}')\n",
    "print(f'Standard Deviation of Difference: {std_diff}')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8af8e03f10>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f8acb89b430, raw_cell=\"import numpy as np\n",
      "import pandas as pd\n",
      "import os \n",
      "..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcn-h002.server.mila.quebec/home/mila/a/armin.moradi/CulturalDiscoverability/notebooks/evaluate_baseline.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data interactions: 1485540 users 8418 items 63076\n",
      "--------------------------------------------------\n",
      "Sampled Interactions 1485540\n",
      "Added Negative Samples ~ Added Items: 1485540\n",
      "--------------------------------------------------\n",
      "FINAL DF    user_id  item_id  count\n",
      "0        0        0      1\n",
      "1        0       65      1\n",
      "2        0       98      1\n",
      "3        0      109      1\n",
      "4        0      124      1\n",
      "x    user_id  item_id\n",
      "0        0        0\n",
      "1        0       65\n",
      "2        0       98\n",
      "3        0      109\n",
      "4        0      124\n",
      "y 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: count, dtype: int64\n",
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f8af8e03f10>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f8acb89b3d0, execution_count=79 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f8acb89b430, raw_cell=\"import numpy as np\n",
      "import pandas as pd\n",
      "import os \n",
      "..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcn-h002.server.mila.quebec/home/mila/a/armin.moradi/CulturalDiscoverability/notebooks/evaluate_baseline.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATASET_PATH = '/home/mila/a/armin.moradi/ProtoMF/data/lfm2b-1mon'\n",
    "ITEM_FRACTION = 1.0\n",
    "USER_FRACTION = 1.0\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "HIDDEN_SIZE = [16, 32, 64]\n",
    "EMBEDDING_SIZE = 8\n",
    "\n",
    "\n",
    "\n",
    "DATASET_PATH = '/home/mila/a/armin.moradi/CulturalDiscoverability/ProtoMF/data/lfm2b-1mon'\n",
    "interactions = pd.read_csv(os.path.join(DATASET_PATH, 'listening_history_train.csv'))\n",
    "user_demographics = pd.read_csv(os.path.join(DATASET_PATH, 'user_ids.csv'))\n",
    "item_demographics = pd.read_csv(os.path.join(DATASET_PATH, 'item_ids.csv'))\n",
    "\n",
    "print('Loaded Data', 'interactions:', len(interactions), 'users', len(user_demographics), 'items', len(item_demographics))\n",
    "print('-'*50)\n",
    "\n",
    "interactions = interactions[['user_id', 'item_id']]\n",
    "interactions['count'] = 1\n",
    "interactions.head()\n",
    "\n",
    "# take only 10 interactions\n",
    "# interactions = interactions.sample(frac=0.01, random_state=42) #TODO REMOVE\n",
    "\n",
    "\n",
    "print('Sampled Interactions', len(interactions))\n",
    "df_final = interactions\n",
    "\n",
    "user_ids = df_final['user_id'].unique()\n",
    "item_ids = df_final['item_id'].unique()\n",
    "\n",
    "negative_samples = []\n",
    "for user_id in user_ids:\n",
    "    user_interacted_items = interactions[interactions['user_id'] == user_id]['item_id'].values\n",
    "    user_negative_items = np.setdiff1d(item_ids, user_interacted_items)\n",
    "    user_negative_items = np.random.choice(user_negative_items, size=len(user_interacted_items), replace=False)\n",
    "    for negative_item in user_negative_items:\n",
    "        negative_samples.append([user_id, negative_item, 0])\n",
    "\n",
    "negative_interactions = pd.DataFrame(negative_samples, columns=['user_id', 'item_id', 'count'])\n",
    "# negative_df = negative_interactions.merge(user_demographics, on='user_id', how='inner')\n",
    "df_final = pd.concat([df_final, negative_interactions], ignore_index=True)\n",
    "\n",
    "print('Added Negative Samples', '~ Added Items:', len(df_final[df_final['count'] == 0]))\n",
    "print('-'*50)\n",
    "\n",
    " # drop old_user_id \n",
    "# df_final = df_final.drop(columns=['old_user_id'])\n",
    "\n",
    "# add up the columns with repeated user_id and item_id and count\n",
    "df_final = df_final.groupby(['user_id', 'item_id']).sum().reset_index()\n",
    "print('FINAL DF', df_final.head())\n",
    "\n",
    "y = df_final['count']\n",
    "x = df_final.drop(['count'], axis=1)\n",
    "\n",
    "print('x', x.head())\n",
    "print('y', y.head())\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "# columns_to_encode = x.columns[2:]\n",
    "# encoder.fit(x[columns_to_encode])\n",
    "# encoded_x = encoder.transform(x[columns_to_encode])\n",
    "# encoded_x = pd.DataFrame(encoded_x, columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "# x = pd.concat([x[['user_id', 'track_id']], encoded_x], axis=1)\n",
    "\n",
    "# user_country_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_country_')]\n",
    "# artist_country_indices = [i for i, col in enumerate(x.columns) if col.startswith('artist_country_')]\n",
    "# user_gender_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_gender_')]\n",
    "# artist_gender_indices = [i for i, col in enumerate(x.columns) if col.startswith('artist_gender')]\n",
    "# user_age_indices = [i for i, col in enumerate(x.columns) if col.startswith('user_age_')]\n",
    "# unique_user_ids, unique_track_ids = max(x['user_id']) + 1, max(x['track_id']) + 1\n",
    "\n",
    "# split the interactions of each user into train, val, and test (80-10-10)\n",
    "\n",
    "train_share = 0.75\n",
    "val_share = 0.15\n",
    "test_share = 1 - train_share - val_share\n",
    "\n",
    "user_interactions = {}\n",
    "for i in range(len(x)):\n",
    "    user_id = x.iloc[i]['user_id']\n",
    "    if user_id not in user_interactions:\n",
    "        user_interactions[user_id] = []\n",
    "    user_interactions[user_id].append(i)\n",
    "\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "for user_id, indices in user_interactions.items():\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices += indices[:int(train_share * len(indices))]\n",
    "    val_indices += indices[int(train_share * len(indices)):int((train_share + val_share ) * len(indices))]\n",
    "    test_indices += indices[int((1 - test_share) * len(indices)):]\n",
    "    \n",
    "train_x, train_y = x.iloc[train_indices], y.iloc[train_indices]\n",
    "val_x, val_y = x.iloc[val_indices], y.iloc[val_indices]\n",
    "test_x, test_y = x.iloc[test_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x.head()\n",
    "\n",
    "\n",
    "print(len(train_x), len(val_x), len(test_x))\n",
    "# create dataloaders\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))\n",
    "val_dataset = TensorDataset(torch.tensor(val_x.values, dtype=torch.float), torch.tensor(val_y.values, dtype=torch.float))\n",
    "test_dataset = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f'created loaders with batchsize {BATCH_SIZE}', len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "\n",
    "# set up wandb logging\n",
    "\n",
    "wandb.init(project='fooling-around', entity='armornine')\n",
    "print('Initialized WandB')\n",
    "\n",
    "unique_user_ids = df_final['user_id'].nunique()\n",
    "unique_track_ids = df_final['item_id'].nunique()\n",
    "\n",
    "class AgnosticNCF(nn.Module):\n",
    "    def __init__(self, num_users, num_tracks, hidden_size=[16, 32, 32]): # 256, 128, 64\n",
    "        super(AgnosticNCF, self).__init__()\n",
    "\n",
    "        self.embedding_size = 8 # 32\n",
    "\n",
    "        self.user_id_embedding = nn.Embedding(num_users, self.embedding_size)\n",
    "        self.track_id_embedding = nn.Embedding(num_tracks, self.embedding_size)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.embedding_size * 2, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[2], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_id, artist_id):\n",
    "\n",
    "        user_id_embeds = self.user_id_embedding(user_id)\n",
    "        track_id_embeds = self.track_id_embedding(artist_id)\n",
    "        \n",
    "\n",
    "        concatenated = torch.cat([user_id_embeds, track_id_embeds], dim=1)\n",
    "        output = self.fc_layers(concatenated.float())\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "print('Start Training')\n",
    "print('-'*50)\n",
    "\n",
    "start_time = time.time()\n",
    "model = AgnosticNCF(num_users=unique_user_ids, num_tracks=unique_track_ids, hidden_size=[64, 128, 64]).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xx, yy in train_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        # user_country = xx[:, user_country_indices].long().to(device)\n",
    "        # artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_id, track_id)# , user_country, artist_country)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    wandb.log({'train_loss': total_loss / len(train_loader)})\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    for xx, yy in val_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        # user_country = xx[:, user_country_indices].long().to(device)\n",
    "        # artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_val_loss += loss.item()\n",
    "    wandb.log({'val_loss': total_val_loss / len(val_loader)})\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(train_loader)}, Val Loss: {total_val_loss / len(val_loader)}')\n",
    "    train_loss.append(total_loss / len(train_loader))\n",
    "    val_loss.append(total_val_loss / len(val_loader))\n",
    "\n",
    "    torch.save(model.state_dict(), LOGS_PATH + 'model_weights.pth')\n",
    "\n",
    "print('Training Time:', round((time.time() - start_time)/60, 2), 'minutes')\n",
    "print('-'*50)\n",
    "\n",
    "# test\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "diff_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xx, yy in test_loader:\n",
    "        user_id = xx[:, 0].long().to(device)\n",
    "        track_id = xx[:, 1].long().to(device)\n",
    "\n",
    "        # user_country = xx[:, user_country_indices].long().to(device)\n",
    "        # artist_country = xx[:, artist_country_indices].long().to(device)\n",
    "\n",
    "        outputs = model(user_id, track_id)\n",
    "        loss = criterion(outputs.float(), yy.float().to(device))\n",
    "        total_test_loss += loss.item()\n",
    "        diff = abs(outputs - yy.float().to(device))\n",
    "        diff_list.append(diff)\n",
    "        wandb.log({'diff mean': diff.mean().item()})\n",
    "        wandb.log({'diff std': diff.std().item()})\n",
    "    wandb.log({'test_loss': total_test_loss / len(test_loader)})\n",
    "    \n",
    "mean_diff = torch.cat(diff_list).mean().item()\n",
    "std_diff = torch.cat(diff_list).std().item()\n",
    "\n",
    "print(f'Test Loss: {total_test_loss / len(test_loader)}')\n",
    "print(f'Mean Difference: {mean_diff}')\n",
    "print(f'Standard Deviation of Difference: {std_diff}')\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
